{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T22:00:13.876812Z",
     "start_time": "2020-04-22T22:00:13.871690Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=50\n",
    "CROP_SIZE=256\n",
    "\n",
    "TRAIN_DATA_PATH = \"./Confocal_BPAE_B/raw\"\n",
    "GT_DATA_PATH = \"./Confocal_BPAE_B/gt\"\n",
    "NOISE_WEIGHT = .4\n",
    "\n",
    "VALIDATION_PCT = .2\n",
    "RANDOM_SEED = 42\n",
    "SHUFFLE_DATASET = False\n",
    "\n",
    "ITERATIONS=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T22:00:14.484674Z",
     "start_time": "2020-04-22T22:00:08.882Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "from numpy import inf\n",
    "from skimage.metrics import structural_similarity as ssm\n",
    "from scipy.special import factorial\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "import torch.nn as nn\n",
    "from torch.nn import MSELoss\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from models.RIM_v3 import RIM\n",
    "from util import show, plot_images, plot_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T21:54:16.071098Z",
     "start_time": "2020-04-22T21:54:16.048844Z"
    }
   },
   "outputs": [],
   "source": [
    "class gtmatch():\n",
    "    def __init__(self, dataset1, dataset2, batch_size):\n",
    "        self.dataset1 = dataset1\n",
    "        self.dataset2 = dataset2\n",
    "        self.batch_size=batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.dataset1[index][0]\n",
    "        y = self.dataset2[int(index / self.batch_size % self.batch_size)][0]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset1)\n",
    "    \n",
    "  \n",
    "\n",
    "class log_funcs():\n",
    "    def __init__(self, updated, noise_ref, h):\n",
    "        self.updated=updated\n",
    "        self.noise_ref=noise_ref\n",
    "        self.h=h\n",
    "\n",
    "    def poisson_log(Ax, b, shift):\n",
    "        # from https://stanford.edu/class/ee367/reading/lecture10_notes.pdf\n",
    "        term1 = torch.sum(np.log(Ax+shift)*b)\n",
    "        term2 = torch.sum(Ax+shift)\n",
    "        term3 = torch.sum(torch.from_numpy(np.log(factorial(b))))\n",
    "        loglikelihood = term1 - term2 - term3\n",
    "        return loglikelihood\n",
    "    \n",
    "    def gradient(updated, noise_ref, h):\n",
    "        grad_ll = []\n",
    "        one_matrix = torch.ones([noise_ref.shape[2], noise_ref.shape[3]], dtype=torch.double)\n",
    "\n",
    "\n",
    "        for i, b in enumerate(noise_ref):\n",
    "            Ax = updated[i]\n",
    "            Ax[Ax <= 0] = 10*h\n",
    "            # Poisson log-likelihood\n",
    "\n",
    "            loglikelihood_del = log_funcs.poisson_log(Ax, b, h)\n",
    "            loglikelihood = log_funcs.poisson_log(Ax, b, 0)\n",
    "\n",
    "            grad = (loglikelihood_del-loglikelihood)/(h)\n",
    "\n",
    "            grad_ll.append(grad.item()*one_matrix)\n",
    "        return torch.stack(grad_ll).unsqueeze(1).float(), loglikelihood\n",
    "\n",
    "\n",
    "        \n",
    "def add_noise(clean_images,noise_weight,noise_type):\n",
    "    clean_np = clean_images.detach().numpy()\n",
    "\n",
    "    if 'Poisson' in noise_type:\n",
    "        noise_mask = np.random.poisson(clean_np)\n",
    "    else:\n",
    "        noise_mask= clean_np*0\n",
    "\n",
    "    noisy_images = torch.clamp(torch.from_numpy(clean_np+(noise_mask*noise_weight)),0,1).float() \n",
    "    return noisy_images, noise_mask\n",
    "\n",
    "\n",
    "def live_plot(data, figsize=(7,5), title=''):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(data)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('Iteration')\n",
    "    clear_output(wait=True)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T21:54:18.074648Z",
     "start_time": "2020-04-22T21:54:18.071322Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T21:54:20.122428Z",
     "start_time": "2020-04-22T21:54:20.070413Z"
    }
   },
   "outputs": [],
   "source": [
    "# load and sort dataset\n",
    "TRANSFORM_IMG = transforms.Compose([\n",
    "    transforms.CenterCrop(CROP_SIZE),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_data = torchvision.datasets.ImageFolder(\n",
    "    root=TRAIN_DATA_PATH, transform=TRANSFORM_IMG)\n",
    "gt_data = torchvision.datasets.ImageFolder(\n",
    "    root=GT_DATA_PATH, transform=TRANSFORM_IMG)\n",
    "\n",
    "newdataset = gtmatch(train_data, gt_data, BATCH_SIZE)\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(newdataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(VALIDATION_PCT * dataset_size))\n",
    "\n",
    "\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "if SHUFFLE_DATASET:\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    np.random.shuffle(train_indices)\n",
    "    np.random.shuffle(val_indices)\n",
    "\n",
    "    \n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SequentialSampler(train_indices)\n",
    "valid_sampler = SequentialSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    newdataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(newdataset, batch_size=BATCH_SIZE,\n",
    "                                                sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T21:54:22.125345Z",
     "start_time": "2020-04-22T21:54:22.117185Z"
    }
   },
   "outputs": [],
   "source": [
    "model = RIM()\n",
    "loss_function = MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T21:55:57.769740Z",
     "start_time": "2020-04-22T21:54:24.133590Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "avg_grad_logs = []\n",
    "val_losses = []\n",
    "\n",
    "best_val_loss = 1\n",
    "st1 = 0\n",
    "st2 = 0\n",
    "loss = 'null'\n",
    "\n",
    "data_loader = train_loader\n",
    "\n",
    "\n",
    "for i, batch in enumerate(data_loader):\n",
    "    model.train()\n",
    "\n",
    "    clean_images, gt = batch\n",
    "    noisy_images, noise_mask = add_noise(clean_images,NOISE_WEIGHT,'Poisson')\n",
    "    input_images = noisy_images\n",
    "    \n",
    "    for j in range(0, ITERATIONS):\n",
    "        \n",
    "        if j ==0:    \n",
    "            grad_log=torch.zeros(noisy_images.shape)\n",
    "            \n",
    "        #xt\n",
    "        net_input=torch.cat([input_images, grad_log], dim=1)\n",
    "        \n",
    "        net_output, st1, st2=model(net_input, st1, st2, j)\n",
    "        \n",
    "        #xt+1 = xt + Î”x\n",
    "        pred_images=input_images+net_output\n",
    "        pred_images=torch.clamp(pred_images,0,1)\n",
    "                \n",
    "        grad_log, loglikelihood=log_funcs.gradient(torch.clamp(pred_images.detach()+noise_mask,0,1),noisy_images,.00001)\n",
    "    \n",
    "        loss=loss_function(pred_images,clean_images)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        avg_grad_logs.append(torch.mean(grad_log).item())\n",
    "        \n",
    "        st1.detach_()\n",
    "        st2.detach_()\n",
    "        pred_images.detach_()\n",
    "        \n",
    "        #xt+1 -> xt\n",
    "        input_images= pred_images\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        live_plot(losses,title=\"Loss Function\")\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        show(np.clip(pred_images.detach().cpu().numpy()[0,0].astype(np.float64),0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T21:55:57.787569Z",
     "start_time": "2020-04-22T21:54:10.890Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_output = []\n",
    "Input_PSNR = []\n",
    "Model_PSNR = []\n",
    "\n",
    "model_output = []\n",
    "Input_PSNR = []\n",
    "Model_PSNR = []\n",
    "Model_loss = []\n",
    "clipped_model_loss = []\n",
    "\n",
    "for i, batch in enumerate(validation_loader):\n",
    "\n",
    "\n",
    "    clean_images, gt = batch\n",
    "    \n",
    "    clean_np = clean_images.detach().numpy()\n",
    "    noise_mask = np.random.poisson(clean_np)\n",
    "    noisy_images = torch.from_numpy(clean_np+noise_mask*NOISE_WEIGHT).float()\n",
    "    net_input=torch.cat([noisy_images, grad_log], dim=1)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    denoised = noisy_images+model(net_input, st1, st2, 0)[0].detach()\n",
    "    denoised_arr = np.clip(denoised.cpu().numpy()[\n",
    "        0, 0].astype(np.float64),0,1)\n",
    "\n",
    "    net_input_np = noisy_images.cpu().numpy()[0, 0].astype(np.float64)\n",
    "    clean_comp = clean_images.detach().cpu().numpy()[\n",
    "        0, 0].astype(np.float64)\n",
    "    Input_PSNR.append(ssm(clean_comp, net_input_np))\n",
    "    Model_PSNR.append(ssm(clean_comp, denoised_arr))\n",
    "    model_output.append([torch.from_numpy(clean_comp),torch.from_numpy(net_input_np), torch.from_numpy(\n",
    "        denoised_arr), gt])\n",
    "\n",
    "    Model_loss.append(loss_function(denoised, clean_images))\n",
    "    clipped_model_loss.append(loss_function(\n",
    "        torch.from_numpy(denoised_arr), clean_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T21:55:57.804740Z",
     "start_time": "2020-04-22T21:54:11.697Z"
    }
   },
   "outputs": [],
   "source": [
    "idx=1\n",
    "plot_tensors(model_output[idx], [\"Origin\",\"Input\", \"Simple RIM\",\"Ground Truth\"])\n",
    "print(\"Input SSIM = \", np.round(Input_PSNR[idx], 2))\n",
    "print(\"Simple RIM SSIM = \", np.round(Model_PSNR[idx], 4))\n",
    "print(\"\")\n",
    "print(\"Simple RIM Loss = \", np.round(Model_loss[idx], 4))\n",
    "print(\"Clipped Simple RIM Loss = \", np.round(clipped_model_loss[idx], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T19:57:36.209129Z",
     "start_time": "2020-04-22T19:57:08.154Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Average Input SSIM = \", np.round(np.mean(Input_PSNR), 2))\n",
    "print(\"Average Simple RIM SSIM = \", np.round(np.mean(Model_PSNR), 4))\n",
    "print(\"\")\n",
    "print(\"Average Simple RIM Loss = \", np.round(np.mean(Model_loss), 4))\n",
    "print(\"Average Clipped Simple RIM Loss = \", np.round(np.mean(clipped_model_loss), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RIM with J-Invariant Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T19:57:36.210283Z",
     "start_time": "2020-04-22T19:57:08.159Z"
    }
   },
   "outputs": [],
   "source": [
    "from mask_RIM_v3 import Masker\n",
    "masker = Masker(width=4, mode='interpolate')\n",
    "\n",
    "data_loader = train_loader\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "clean_image_corr = []\n",
    "best_images = []\n",
    "noisy_input = []\n",
    "best_Jimages = []\n",
    "best_val_loss = 1\n",
    "st1=0\n",
    "st2=0\n",
    "\n",
    "for j in range(0,5):\n",
    "    \n",
    "    print(j)\n",
    "\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        clean_images, gt = batch\n",
    "\n",
    "        noise=torch.distributions.poisson.Poisson(clean_images.detach()).sample()*noise_weight\n",
    "        gaussian=torch.randn(clean_images.size())*noise_weight\n",
    "        noisy_images=clean_images+noise\n",
    "        \n",
    "\n",
    "\n",
    "        model.train()\n",
    "\n",
    "\n",
    "        if i==0:\n",
    "            loglikelihood=torch.zeros(noisy_images.shape)\n",
    "            \n",
    "        net_input=torch.cat([noisy_images, loglikelihood],dim=1)\n",
    "\n",
    "            \n",
    "        net_input, mask = masker.mask(net_input, i % (masker.n_masks - 1))\n",
    "\n",
    "\n",
    "        net_output, st1, st2 = model(net_input, st1, st2, i, j)\n",
    "        \n",
    "        st1=st1.detach()\n",
    "        st2=st2.detach()\n",
    "\n",
    "\n",
    "        pred_image=net_output+noisy_images\n",
    "        loglikelihood=(pred_image+noise)-noisy_images\n",
    "        loglikelihood=loglikelihood.detach()\n",
    "\n",
    "        loss = loss_function(pred_image*mask, noisy_images*mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = loss_function(pred_image*mask, noisy_images*mask)\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            denoised = noisy_images+model(net_input, st1, st2, i, j)[0].detach()\n",
    "            Jinv = noisy_images + \\\n",
    "                masker.infer_full_image(net_input, model, st1, st2, i, j)[0].detach()\n",
    "            Jinv = np.clip(Jinv, 0, 1).cpu().numpy()[\n",
    "                0, 0].astype(np.float64)\n",
    "            net_input_np = noisy_images.cpu().numpy()[0, 0].astype(np.float64)\n",
    "            denoised_arr = np.clip(denoised, 0, 1).cpu().numpy()[\n",
    "                0, 0].astype(np.float64)\n",
    "            clean_comp = clean_images.detach().cpu().numpy()[\n",
    "                0, 0].astype(np.float64)\n",
    "\n",
    "            best_psnr = compare_psnr(clean_comp, Jinv)\n",
    "\n",
    "            noisy_input.append(net_input_np)\n",
    "            best_images.append(denoised_arr)\n",
    "            best_Jimages.append(Jinv)\n",
    "            clean_image_corr.append(clean_images)\n",
    "            print(\"Loss (\", i,\"-\",j, \"): \\t\", round(loss.item(), 4))\n",
    "            print(\"\\tModel PSNR: \", np.round(best_psnr, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T19:57:36.211392Z",
     "start_time": "2020-04-22T19:57:08.163Z"
    }
   },
   "outputs": [],
   "source": [
    "model_output = []\n",
    "Input_PSNR = []\n",
    "Model_PSNR = []\n",
    "JInv_PSNR = []\n",
    "Model_loss = []\n",
    "clipped_model_loss = []\n",
    "\n",
    "JInv_loss = []\n",
    "clipped_JInv_loss = []\n",
    "\n",
    "for i, batch in enumerate(validation_loader):\n",
    "\n",
    "    clean_images, gt = batch\n",
    "    \n",
    "    noise=torch.distributions.poisson.Poisson(clean_images.detach()).sample()*noise_weight\n",
    "    gaussian=torch.randn(clean_images.size())*noise_weight\n",
    "    noisy_images=clean_images+noise\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    net_input=torch.cat([noisy_images, loglikelihood],dim=1)\n",
    "\n",
    "    denoised = noisy_images+model(net_input, st1, st2, i, j)[0].detach()\n",
    "    denoised_arr = np.clip(denoised, 0, 1).cpu().numpy()[\n",
    "        0, 0].astype(np.float64)\n",
    "    Jinv = noisy_images + \\\n",
    "        masker.infer_full_image(net_input, model, st1, st2, i, j)[0].detach()\n",
    "    Jinv_np = np.clip(Jinv, 0, 1).cpu().numpy()[\n",
    "        0, 0].astype(np.float64)\n",
    "\n",
    "    net_input_np = noisy_images.cpu().numpy()[0, 0].astype(np.float64)\n",
    "    clean_comp = clean_images.detach().cpu().numpy()[\n",
    "        0, 0].astype(np.float64)\n",
    "\n",
    "    model_output.append([torch.from_numpy(clean_comp),torch.from_numpy(net_input_np), torch.from_numpy(\n",
    "        denoised_arr), torch.from_numpy(Jinv_np), gt])\n",
    "    Input_PSNR.append(compare_psnr(clean_comp, net_input_np))\n",
    "    Model_PSNR.append(compare_psnr(clean_comp, denoised_arr))\n",
    "    JInv_PSNR.append(compare_psnr(clean_comp, Jinv_np))\n",
    "    \n",
    "    Model_loss.append(loss_function(denoised, noisy_images))\n",
    "    clipped_model_loss.append(loss_function(\n",
    "        torch.from_numpy(denoised_arr), noisy_images))\n",
    "    \n",
    "    JInv_loss.append(loss_function(Jinv, noisy_images))\n",
    "    clipped_JInv_loss.append(loss_function(\n",
    "        torch.from_numpy(Jinv_np), noisy_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T19:57:36.212490Z",
     "start_time": "2020-04-22T19:57:08.166Z"
    }
   },
   "outputs": [],
   "source": [
    "idx=1\n",
    "plot_tensors(model_output[idx], [\"Origin\",\"Input\", \"Simple RIM\",\n",
    "                               \"J-Invariant RIM\", \"Ground Truth\"])\n",
    "print(\"Input PSNR = \", np.round(Input_PSNR[idx], 2))\n",
    "print(\"Simple RIM PSNR = \", np.round(Model_PSNR[idx], 4))\n",
    "print(\"J-Invariant RIM PSNR = \", np.round(JInv_PSNR[idx], 4))\n",
    "print(\"\")\n",
    "print(\"Simple RIM Loss = \", np.round(Model_loss[idx], 4))\n",
    "print(\"Clipped Simple RIM Loss = \", np.round(clipped_model_loss[idx], 4))\n",
    "print(\"\")\n",
    "print(\"J-Invariant RIM Loss = \", np.round(JInv_loss[idx], 4))\n",
    "print(\"Clipped J-Invariant RIM Loss = \", np.round(clipped_JInv_loss[idx], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T19:57:36.213570Z",
     "start_time": "2020-04-22T19:57:08.169Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Average Input PSNR = \", np.round(np.mean(Input_PSNR), 2))\n",
    "print(\"Average Simple RIM PSNR = \", np.round(np.mean(Model_PSNR), 4))\n",
    "print(\"Average J-Invariant RIM PSNR = \", np.round(np.mean(JInv_PSNR), 4))\n",
    "print(\"\")\n",
    "print(\"Average Simple RIM Loss = \", np.round(np.mean(Model_loss), 4))\n",
    "print(\"Average Clipped Simple RIM Loss = \", np.round(np.mean(clipped_model_loss), 4))\n",
    "print(\"\")\n",
    "print(\"Average J-Invariant RIM Loss = \", np.round(np.mean(JInv_loss), 4))\n",
    "print(\"Average Clipped J-Invariant RIM Loss = \", np.round(np.mean(clipped_JInv_loss), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T19:57:36.214694Z",
     "start_time": "2020-04-22T19:57:08.174Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.special import gamma, factorial\n",
    "[1,2,3]-sum(factorial([1,2,3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T19:57:36.215872Z",
     "start_time": "2020-04-22T19:57:08.178Z"
    }
   },
   "outputs": [],
   "source": [
    "[1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T19:57:36.217029Z",
     "start_time": "2020-04-22T19:57:08.181Z"
    }
   },
   "outputs": [],
   "source": [
    "NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
